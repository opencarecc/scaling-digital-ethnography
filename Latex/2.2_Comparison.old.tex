\subsection{A comparison}

\begin{enumerate}
\item SMT generally collects evidence that was generated before, and independently of, the research itself. CE evidence are instead normally generated at least in part for the purpose of the research, and by informants who are aware of the research.

\item SMT collects social media content, whereas CE collects ethnographic data in a more classic sense. Ethnographic researchers, when in the field, operate within a context shared with their sources ("informants" in ethnography parlance). They can backtrack, ask questions, disambiguate. The ethnographer can interact with informants: ask them to expand upon a thought, clarify their meaning or use of certain vocabulary, or promote deeper discussion of particular topics. The encoded assumptions of the ethnographer do not persist untested and unchallenged, but instead are actively reworked through discussions with informants and adapted through immersion in their evolving environment.  

None of this is possible when they are studying content that was produced prior to ethnographic research. As a result, the signal-to-noise ratio of most digital ethnography is quite low. Extracting any meaningful information about the issue being investigated can be a real challenge. Much of the content trawled is user-syncratic (reflects the world view of one single person, thereby negating the "ethno" in ethnography \cite{Munk2016}) or media-syncratic (reflects the world view implicit in the affordances or corporate strategies of the social media platform being trawled \cite{Gershon2011,Bucher2017,Burrell2012,Munk2016}. This problem is much less present with the CE approach, where informants co-construct and sustain visible themes of conversation through interaction with the ethnographic researcher. Further, when an ethnographer is synchronically doing research with informants, she can comprehend and contextualise the temporal unfolding of information rather than getting lost in noise~\cite{Coleman2010}.

\item SMT can compensate for the low signal-to-noise ratio by harvesting more data from the same social media. Harvesting is a scalable activity, so this is cost-effective. However, dealing with a lot of low-quality data makes ethnographic coding by humans inefficient and unrealistic. The logical step for SMT researchers is then to deploy the tagging practice of many social media ("tags" for blogs, "hastags" for Twitter etc.) as a crowd-sourced substitute for ethnographic codes. However, the substitution is not perfect. Ontologies of ethnographic codes are created by professional researchers in a coordinated effort with informants, and carefully maintained; folksonomies of tags are created in an uncoordinated way by non-researchers who are not concerned about maintaining consistency~\cite{Stilo2014}. For example, two blog posts could describe exactly the same situation (moving to France to take on a new job) by completely different tags, like \texttt{\#france}, \texttt{\#career} or \texttt{\#expats}. Thanks to its higher signal-to-noise ratio, CE studies can stay small (though still much larger than on-site studies) and bring an emphasis on professional coding. 

\item CE makes explicit use of the conversation across informants as a convergence mechanism. Informants share the same context, and frequently engage with one another. In online forums, informants are put in visible sight of one another and encouraged to interact. Interaction leads to confirming each other's views or disagreeing with them, enriching each other's contributions, disambiguating, and so on. With SMT, there is no guarantee that the informants, whose content was trawled, are aware of each other's position, nor, indeed, that there is a group whose views are being studied, and that they are part of it.

\item With SMT, it is generally impossible or impractical to engage with the informants. The researcher practices non-participant observation. With CE, the researcher can and does engage with the informants, asking questions and encouraging them to provide details when needed. The researcher practices participant observation but the absence of cues from physical appearance, body language, clothing, etc. makes CE more discrete than on-site ethnography: there is no visual cue to tell an ethnographer from any other user of the online forum (but in ethical practice, the ethnographer does disclose herself to informants). The ethnographer becomes part of the fabric of the community she studies, actively participating and allowing that engagement to inform her codes \cite{Goffman1989,Emerson2011}.

% * <alberto.cottica@gmail.com> 2017-05-25T13:27:13.807Z:
% 
% Amelia, is this exclusive of anthro? Does it generalise to all ethno? We should probably say. AH: Yes, can generalise! Have added 2 citations, one on participant-observation from Goffman and one which is from writing/coding fieldnotes book which is gold standard for ethno education, youd find it on the syllabus in most courses, anthro or not
% 
% ^.

\end{enumerate}

Thus ethnographic codes which emerge from extended participant-observation as well as interactions with informants, who themselves have been interacting with each other on a platform, tend to encode greater richness than self-tagging done on social media sites. There is a wealth of scholarship which indicates that social media sites are used in highly selective ways depending on the affordances of the technology and the sociocultural norms surrounding the technology that differ from context to context~\cite{Haythornthwaite2005,Miller2011}. These differences make comparisons of self-tagging across cultural contexts without ethnographic engagement problematic. Further, since users exist in a wide social media environment (what Madianou and Miller term 'polymedia'~\cite{Madianou2012}), analysis of tags from any one media in isolation risks serious incompleteness. Finally, using user-deployed tags without understanding the conditions under which these tags were produced makes it difficult for researchers attempting to glean meaning from tags like \texttt{nature} and \texttt{fitness}, concepts which differ greatly depending on sociocultural context.

Ethnographic coding done on a bounded virtual field site, mirroring anthropological engagement offline, offers ways to mitigate these drawbacks while maintaining the ability to analyse large quantities of digital data. As themes begin to emerge, the ethnographer has the ability to edit tags, adding more nuance. Thus the ethnographic coding process is both cumulative and recursive. It is also consistent: rather than having different terms for the same phenomenon (e.g. \texttt{migration}; \texttt{refugees}; \texttt{people on the move}) that create noise, the ethnographer streamlines the tags. This curation means that distinctions between two terms imply a clear difference, e.g., \texttt{resilience} in the OpenCare dataset is different from \texttt{sustainability}, which means that informants on the platform distinguish between these concepts in meaningful ways. Noise is reduced so productive difference can stand out more clearly.

It is essential to note that this ethnographic process can scale due to the help of digital support systems. In our case, the online discussions concerning OpenCare are handled on Edgeryders,\footnote{\url{https://edgeryders.eu}} an online interaction environment structured like a forum. We additionally use Open Ethnographer, a software to encode and preserve the ethnographer's annotations and tags, while providing ways to merge, fork, and rename them. An online visualisation platform, named Graph Ryder, also allows the ethnographer, as well as informants and team members, to view co-occurrences of annotations and visualise the data in different ways that can provide new insights. These insights then recursively influence future coding in a classic visualisation-interaction feedback loop~\cite{Wijk2005} as tags are continuously re-evaluated to improve accuracy and increase understanding of the interactions onsite.
